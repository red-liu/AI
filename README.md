|    日期  |    主题  |   知识点详情 | 课件  |  相关阅读  |   其 他 | 作业 |
|---------|---------|---------|---------|---------|---------|---------|
| PART 1 |
| 6.29   | （直播-Lecture）NLP概览、数学基础 | NLP应用概览、NLP技术概览、统计机器翻译、贝叶斯定理|[Lecture1](http://47.94.6.102/NLPCamp4/Lecture1-Introduction)|[Git的使用](https://www.greedyai.com/course/46)<br/>[Statistical Machine Translation](http://www.cse.aucegypt.edu/~rafea/CSCE569/slides/Statistical%20Machine%20Translation.pdf)||[第一次小作业](http://47.94.6.102/NLPCamp4/WeeklyAssignments) Out, 截止日期：7月13日（周六）北京时间 23:59PM <br/> [第一篇论文阅读](https://www.cs.ucr.edu/~eamonn/SIGKDD_trillion.pdf) 截止日期：7月13日（周六）北京时间 23:59PM，发送链接给班主任 <br/> [如何写Paper Summary, 如何提交？](http://47.94.6.102/NLPCamp4/course-info/wikis/Paper-Summary%E6%80%8E%E4%B9%88%E5%86%99%EF%BC%9F)<br/> <br/>[如何提交小作业？](http://47.94.6.102/NLPCamp4/WeeklyAssignments/wikis/%E5%B0%8F%E4%BD%9C%E4%B8%9A%E7%9A%84%E6%8F%90%E4%BA%A4%E4%B8%8E%E6%B5%81%E7%A8%8B)|
| 7.2    | （直播-Lecture）算法复杂度，动态规划 |时间/空间复杂度、Master's Theorem, 递归程序的分析与栈、动态规划算法、编辑距离|[Lecture2](http://47.94.6.102/NLPCamp4/Lecture2-Complexity)|[[博客]十分钟搞定时间复杂度](https://www.jianshu.com/p/f4cca5ce055a)<br/>[[博客] Dynamic Programming – Edit Distance Problem](https://algorithms.tutorialhorizon.com/dynamic-programming-edit-distance-problem/)<br/>[[材料]Master's Theorem](http://people.csail.mit.edu/thies/6.046-web/master.pdf)<br/>[Introduction to Algorithm (MIT Press)](http://ressources.unisciel.fr/algoprog/s00aaroot/aa00module1/res/%5BCormen-AL2011%5DIntroduction_To_Algorithms-A3.pdf)|||
| 7.6    | （直播-Lecture）逻辑回归与正则 |逻辑回归模型，SGD，过拟合与正则, L1,L2, Hyperparameter Tuning|[Lecture3](http://47.94.6.102/NLPCamp4/Lecture3-DTW-LR-Regularization)|[Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)<br/>[ElasticNet](https://web.stanford.edu/~hastie/Papers/elasticnet.pdf)<br/>[Using the Triangle Inequality to Accelerate Kmeans](http://cseweb.ucsd.edu/~elkan/kmeansicml03.pdf)<br/>[$1 Unistroke Recognizer](http://depts.washington.edu/ilab/proj/dollar/index.html)<br/>[Metric Space](https://www.math.ucdavis.edu/~hunter/m125a/intro_analysis_ch7.pdf)|||
| 7.7    | （直播-Paper） [Searching and Mining Trillions of Time Series Subsequences under Dynamic Time Warping](https://www.cs.ucr.edu/~eamonn/SIGKDD_trillion.pdf)<br/>  ||[pdf课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/DTW%20.pdf)||||
| 7.7    | （直播-Discussion）动态规划问题实战 |常见动态规划问题解析|[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.7%E8%AF%BE%E7%A8%8B%E4%BB%A3%E7%A0%81)|[[博客]Dynamic Programming Practice Problems](https://people.cs.clemson.edu/~bcdean/dp_practice/)|||
| 7.9    |  (直播-Lecture) SVM模型 | 线性SVM，KKT, 对偶 |[Lecture4](http://47.94.6.102/NLPCamp4/Lecture4-LinearSVM) ||||
| 7.13   | （直播-Discussion）集成机器学习模型 | GBDT以及XGBoost |[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/GBDT%20&%20XGBOOST.pptx)|[[Tianqi's Slide]Introduction to Boosted Trees ](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf)|||
| 7.13   | （直播-Lecture） SVM，核函数，决策树，随机深林 || [Lecture5](http://47.94.6.102/NLPCamp4/Lecture5-SVM-DT-RF) ||| <br/ >[第一次小作业](http://47.94.6.102/NLPCamp4/WeeklyAssignments), 截止日期：7月13日（周六）北京时间 23:59PM <br/> [第一篇论文阅读](https://www.cs.ucr.edu/~eamonn/SIGKDD_trillion.pdf) 截止日期：7月13日（周六）北京时间 23:59PM，发送链接给班主任 <br/> [如何写Paper Summary, 如何提交？](http://47.94.6.102/NLPCamp4/course-info/wikis/Paper-Summary%E6%80%8E%E4%B9%88%E5%86%99%EF%BC%9F)<br/> <br/>[如何提交小作业？](http://47.94.6.102/NLPCamp4/WeeklyAssignments/wikis/%E5%B0%8F%E4%BD%9C%E4%B8%9A%E7%9A%84%E6%8F%90%E4%BA%A4%E4%B8%8E%E6%B5%81%E7%A8%8B)|
| 7.14   | （直播-Discussion）核函数（进阶） | 不同的Kernel Function, Mercer's Theorem |[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.14%E8%AF%BE%E4%BB%B6%E4%BB%A3%E7%A0%81)||||
| 7.16   | （直播-Lecture） 凸函数（1） | 凸集，凸函数  |[Lecture6](http://47.94.6.102/NLPCamp4/Lecture6-Convex)|[Convex Optimization by Boyd](https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf)<br/>[Slide by Ducci](https://stanford.edu/~jduchi/PCMIConvex/Duchi16.pdf)||
| 7.17   | （直播-Discussion) Linear Programming实战案例 | |[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/7.18/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92.pptx) |||
| 7.20   | （录播-Discussion) homework1讲解 |  |[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.20)|||
| 7.20   | （直播-Lecture)  凸函数（2） | Lagrangian Duality, Optimization Algorithms, Convergence Analysis, Projected Gradient Descent |[Lecture7](http://47.94.6.102/NLPCamp4/Lecture7-Optimization)|||第二篇论文（XGBoost- A Scalable Tree Boosting System.pdf）summary截止(开课前），把知乎链接发送给班主任老师|
| 7.21   | （直播-Discussion) NP hard问题与Constrained Relaxation | Set Cover Problem讲解 |[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.21%E4%BB%A3%E7%A0%81)|第三篇论文（From Word Embeddings To Document Distances） Due 把知乎链接发送给班主任老师||
| 7.27   | （直播-Discussion) Deep Q Work |  |[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.27%E8%83%A1%E8%80%81%E5%B8%88)|||
| 7.27   | （直播-Discussion) 第三篇论文解读 | WMD  |[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/7.27%E8%83%A1%E8%80%81%E5%B8%88)|||
| 7.27   | （直播-Discussion) 线性规划（2） |Duality,  Simplex算法 |[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E7%BA%BF%E6%80%A7%E8%A7%84%E5%88%92%EF%BC%882%EF%BC%89.pptx)|||
| -----  | ||||||
| PART 2  语言模型与序列标注| ||||||
| 7.30   | (直播-Lecture) 文本表示基础（1） | 分词，拼写纠错，停用词过滤，词的标准化，Viterbi|[Lecture8](http://47.94.6.102/NLPCamp4/Lesson8)|[文本预处理（代码参考）](https://www.kaggle.com/shashanksai/text-preprocessing-using-python)<br/>[分词中的最大匹配算法](https://blog.csdn.net/selinda001/article/details/79345072)<br/> [拼写纠错](https://web.stanford.edu/class/cs124/lec/spelling.pdf)<br/>[Edit Distance](https://www.geeksforgeeks.org/edit-distance-dp-5/)<br/>[DP练习题](https://people.cs.clemson.edu/~bcdean/dp_practice/)<br/>[Porter Stemmer](https://tartarus.org/martin/PorterStemmer/java.txt)<br/>[tf-idf介绍（技术博客）](https://www.cnblogs.com/pinard/p/6693230.html)[Porter Stemming （网页版介绍）]( http://facweb.cs.depaul.edu/mobasher/classes/csc575/papers/porter-algorithm.html)</br>[QuAC : Question Answering in Context](https://arxiv.org/pdf/1808.07036.pdf)<br/>[Coarse-to-Fine Question Answering for Long Documents](https://homes.cs.washington.edu/~eunsol/papers/acl17eunsol.pdf)||第四篇论文：Mining and Summarizing Customer Reviews (should be easy one!)<br/>[第二次小作业](http://47.94.6.102/NLPCamp4/Homework2) posted. Due in 1 week |
| 8.3    | (直播-Lecture）文本表示基础（2） | 词袋模型，文本相似度计算, 词向量，句子向量，倒排表, Noisy Channel Model|[Lecture9](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.3%E6%96%87%E5%93%B2%E8%80%81%E5%B8%88%E8%AF%BE%E7%A8%8B)|[倒排列表（Manning et al. 第一章）](https://nlp.stanford.edu/IR-book/pdf/01bool.pdf)<br/>[余弦相似度介绍（技术博客）](https://blog.csdn.net/ifnoelse/article/details/7766123)</br>[Optimizing Chinese Word Segmentation for Machine Translation Performance（分词，ACL 2008）](https://nlp.stanford.edu/pubs/acl-wmt08-cws.pdf)||1st Programming Project (QA System) Post (Due in 2 weeks)|
| 8.3    | (直播-Discussion)  各类文本相似度计算技术Survey (短文本、长文本） ||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.3)|||
| 8.3    | (直播-Discussion) Glove，ELMo的介绍，以及实战（不涉及过多细节）  ||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.3%E7%9B%BC%E7%9B%BC%E8%80%81%E5%B8%88)|||
| 8.10   | (直播-Discussion) 拼写纠错实战：拼错单词以及语法错误||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.10%E8%83%A1%E7%9B%BC%E7%9B%BC)|||
| 8.11   | (直播-Lecture) EM算法 |Latent Variable Model, EM Derivation, K-means, GMM|[Lecture10](http://47.94.6.102/NLPCamp4/Lesson10-EM)|[The Expectation Maximization Algorithm A short tutorial](http://www.seanborman.com/publications/EM_algorithm.pdf)</br>[Clustering, K-Means, EM Tutorial ](https://www.cs.toronto.edu/~jlucas/teaching/csc411/lectures/tut8_handout.pdf)|第五篇论文：Reading Wikipedia to Answer Open-Domain Questions|
| 8.11   |（直播-Discussion）问答系统前沿技术剖析（state-of-art for QA System) ||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.12)|||
| 8.13   | (直播-Lecture）语言模型 | N-gram, 各类Smoothing算法， Perplexity， Character-n Gram|[Lecture11](http://47.94.6.102/NLPCamp4/Lecture11)|[A Neural Probabilistic Language Model](http://jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)<br/><br/> [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)|||
| 8.17   | (直播-Lecture) HMM | HMM模型介绍，Viterbi Decoding, and Baum Welch算法 |[Lecture12](http://47.94.6.102/NLPCamp4/Lecture12)|[A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition ](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/rabiner.pdf)<br/>[Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/A.pdf)<br/>[如何用简单易懂的例子解释隐马尔可夫模型？](https://www.zhihu.com/question/20962240/answer/33561657)||第一次大的项目作业[Project1](http://47.94.6.102/NLPCamp4/Project1)|
| 8.18   |（直播-Paper) 第五次论文讲解|GloVe: Global Vectors for Word Representation|||
| 8.20   |（直播-Lecture)  HMM（2）|Estimation of HMM Parameters|[Lecture13](http://47.94.6.102/NLPCamp4/Lecture13)|[D-separation](http://www.cs.cmu.edu/~awm/15781/slides/bayesinf05a.pdf)<br/>[Infinite HMM(nonparametric model)](http://mlg.eng.cam.ac.uk/zoubin/papers/ihmm.pdf)<br/>|
| 8.24   |（直播-Discussion）不同的语言模型Smoothing技术 ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Smoothing%E6%8A%80%E6%9C%AF%E7%AE%80%E4%BB%8B.pdf)|||
| 8.24   |（直播-Discussion）基于HMM的词性分析（POS tagger） ||[课件代码](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/8.24%E7%9B%BC%E7%9B%BC%E8%80%81%E5%B8%88%E8%AF%BE%E7%A8%8B)|||
| 8.24   | (直播-Lecture） CRF | Log-Linear Model, linear-CRF，参数估计|[Lecture14](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/crf.zip)|[Log-linear models and conditional </br>random fields</br>](http://cseweb.ucsd.edu/~elkan/250B/CRFs.pdf)，[An Introduction to Conditional</br> Random Fields</br>](http://homepages.inf.ed.ac.uk/csutton/publications/crftut-fnt.pdf)，[Log-Linear Models and Conditional</br> Random Field</br>](http://cseweb.ucsd.edu/~elkan/250Bfall2007/loglinear.pdf)，[Generative Learning algorithms</br>](http://cs229.stanford.edu/notes/cs229-notes2.pdf),[网址]http://videolectures.net/cikm08_elkan_llmacrf/|||
| 8.24   |（直播-Discussion）基于HMM的中文分词： jieba分词原理讲解 ||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/HMM%E5%88%86%E8%AF%8D.zip)|||
| 8.25   |（直播-Paper）第四篇论文讲解《Mining and Summarizing Customer Reviews 》 ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Mining%20and%20Summarizing%20Customer%20Reviews.pptx)|||
|-----|||||||
| 8.29   | (直播-Lecture）crf（2）||[Lecture15](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/crf2.zip)|||
| 8.31   |（直播-Discussion）BiLSTM-CRF的实战|词性标注或者命名实体识别或者NLU为例|[课件代码](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/%E4%BB%A3%E7%A0%818.31)|||
| 8.31   | (直播-Lecture）crf（3）||[Lecture16](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/crf3.zip)|||
| 8.31   |（直播-Discussion）CRF的实战|词性标注或者命名实体为例， 包括特征工程|[课件代码](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/crf%E8%AF%BE%E4%BB%B6%E4%BB%A3%E7%A0%81)|||
| 8.31   |（录播-homework2）|||||
| 9.1    |（直播-Paper）第六篇论文讲解|Real-time Personalization using Embeddings for Search Ranking at Airbnb||||
| 9.3    | (直播-词向量) | 分布式表示，SkipGram以及Derivation |[Lecture17](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/LECTURE17.zip) | [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/abs/1301.3781)</br>[Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546) |||
| 9.7    |（直播-Discussion）Pytorch  Tutorial||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Pytorch.zip)|||
| 9.7    |（直播-Discussion）SkipGram with Negative Sampling代码讲解|梯度下降法实现、Huffman Tree等|[课件代码](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/SkipGram%20with%20Negative%20Sampling%E4%BB%A3%E7%A0%81%E8%AE%B2%E8%A7%A3)|||
| 9.10   | (直播-Lecture）词向量（2）|各类词向量技术，分类 （MF, ELMo, NNLM, Gassian等10多种常见的）|[Lecture18](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/LECTURE18.zip)|||
| 9.17   | (直播-Lecture) 深度学习（1）|多层网络结构， 深度学习和浅层学习， 表示学习的几大特点， BP算法 |[Lecture19](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/lecture19.zip)|[Learning to Disentangle Factors of Variation with Manifold Interaction](http://web.eecs.umich.edu/~honglak/icml2014_disentangling_final.pdf)<br/>[Visualizing and Understanding Convolutional Networks](https://arxiv.org/abs/1311.2901)<br/>[Visualizing and Understanding Neural Models in NLP](https://arxiv.org/abs/1506.01066)<br/>[THE UNIVERSAL APPROXIMATION THEOREM FOR NEURAL NETWORKS](http://mcneela.github.io/machine_learning/2017/03/21/Universal-Approximation-Theorem.html)||
| 9.21   |（直播-Discussion）GPU的使用与环境搭建 +  基于pytorch的简单的神经网络搭建||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/pytorch.zip)|||
| 9.21   | (直播-Lecture) 深度学习（2）|BP算法详解（续）， 深度学习中需要知道的技术， 梯度爆炸|[Lecture20](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/Lecture20)|[Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0)<br/>[Greedy Layer-Wise Training of Deep Networks](http://web1.sph.emory.edu/users/hwu30/teaching/statcomp/papers/BengioNips2006All.pdf)<br/>[Review of Boltzmann machines and simulatedannealin](http://www.cs.utoronto.ca/~yueli/CSC321_UTM_2014_files/tut9.pdf)<br/>[Reducing the Dimensionality of Data with Neural Networks](https://science.sciencemag.org/content/313/5786/504.full)<br/>[The Neural Autoregressive Distribution Estimator](http://proceedings.mlr.press/v15/larochelle11a/larochelle11a.pdf)</br>[Marginalized Denoising Autoencoders for Domain Adaptation](https://arxiv.org/pdf/1206.4683.pdf)</br>[Layer-wise relevance propagation](http://heatmapping.org/)</br>||
| 9.21   |（直播-Discussion）Pytorch讲解2||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Pytorch2.zip)|||
| 9.22   |（直播-Discussion）第七次论文|Representation Learning- A Review and New Perspectives||||
| 9.24   | (直播-Lecture)RNN/LSTM||[Lecture21](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/9.24.pdf)|||
| 9.28   | (直播-Lecture)Jerry老师深度学习方面||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E8%AF%BE%E4%BB%B6%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80)|||
| 9.28   |（直播-Discussion）第八次论文|||||
| 9.29   |（直播-Discussion）利用pytorch实现LSTM||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Pytorch3LSTM.zip)||
| 10.3   | ||||[project 2：Aspect-Based Sentiment Analysis](http://47.94.6.102/NLPCamp4/project2)|
| 10.8   | (直播-Lecture）RNN/LSTM |RNN, Gradient Vanishing, LSTM, Inference, Beam Search|[Lecture22](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Archive.zip)|[LSTM- A Search Space Odyssey(见论文集)]()</br>[Sequence to Sequence Learning with Neural Networks(见论文集)]()</br>[Bidirectional LSTM-CRF Models for Sequence Tagging(见论文集)]()</br>||
| 10.13  | (直播-Discussion）Seq2Seq实战 ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/seq2seq%E5%AE%9E%E8%B7%B5.pdf)|||
| 10.13  |（直播-paper)  Bidirectional LSTM-CRF Models for Sequence Tagging ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Paper13)|||
| 10.19  |（直播-Discussion）Project1实战讲解||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/project1)|||
| 10.19  | (直播-Lecture)Attention||[Lecture23](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Attention.7z)|||
| 10.19  | (直播-Lecture)Self-Attention||[Lecture24](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6)|||
| 10.20  |（直播-paper) |||||
| 10.20  |（直播-Discussion）机器翻译实战||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91)|||
| 10.20  |（直播-Discussion）利用LSTM生成文本||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/%E6%96%87%E6%9C%AC%E7%94%9F%E6%88%90)|||
| 10.22  | (直播-Lecture）Transformer, BERT||[Lecture25]()|||
| 10.26  | (直播-Discussion）BERT的训练与实战||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Review%20Session_20191023_bert%201.pptx)|||
| 10.26  | (直播-Lecture）Xlnet||[Lecture26](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/NLP4_XLNET.zip)|||
| 10.27  |（直播-paper)  BERT- Pre-training of Deep Bidirectional Transformers for Language Understanding.||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/bert%E8%AE%BA%E6%96%87)|||
| 10.27  | (直播-Discussion）基于Transformer的机器翻译||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Transformer%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91.pptx)|||
| 11.2   | (直播-Discussion）XLNet的实战||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/XLNet%E7%AE%80%E4%BB%8B.pdf)|||
| 11.2   | (直播-Lecture）Xlnet（2）||[Lecture27](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/XLNet2.pptx)|||
| 11.3   | (直播-Lecture）PCA||[Lecture28](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/PCA.pptx)|||
| 11.5   | (直播-Lecture）关系抽取1||[Lecture29](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96.zip)|||
| 11.9   |（直播-paper)XLNet: Generalized Autoregressive Pretraining for Language Understanding||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/1109%20nlp4.zip)||||
| 11.9   | (直播-Lecture）关系抽取2||[Lecture30](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%EF%BC%88%E5%91%A8%E5%85%AD%EF%BC%89.zip)|||
| 11.9   | (直播-Discussion）Bert实战2||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/1109panpan/%E8%AF%BE%E4%BB%B6%E4%B8%8B%E8%BD%BD%E5%9C%B0%E5%9D%80)|||[Project3：知识图谱项目](http://47.94.6.102/NLPCamp4/Project3)|
| 11.16  |（直播-Discussion)Relation Extraction with PCNN ||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/11.16review)|||
| 11.16  |（直播-Lecture)关系抽取（3）、 实体统一 ||[Lecture31](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/slide_%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%EF%BC%883%EF%BC%89%E3%80%81%20%E5%AE%9E%E4%BD%93%E7%BB%9F%E4%B8%80.pptx)|||
| 11.17  |（直播-paper)ALBert||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/ALBert)|||
| 11.19  |（直播-Lecture)句法分析 ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/NLP4_%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.zip)|||
| 11.23  |（直播-Discussion)依存文法分析（Dependency Parsing) ||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.pptx)|||
| 11.23  |（直播-Lecture)TransE 和 Node2Vec||[Lecture32](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/ppt.ppt)|||
| 11.24  |（直播-Lecture) 概率图模型1: 贝叶斯估计||[Lecture33](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/slide_%E8%A1%A5%E8%AF%BE%EF%BC%9A%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B1%EF%BC%9A%E8%B4%9D%E5%8F%B6%E6%96%AF%E4%BC%B0%E8%AE%A1.pptx)|[博客](https://zhuanlan.zhihu.com/p/37215276)|||
| 11.26  |（直播-Lecture) 概率图模型2: 主题模型||[Lecture34](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B2%EF%BC%9A%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B.zip)|[LDA paper](http://jmlr.org/papers/volume3/blei03a/blei03a.pdf)||论文：26. Dropout as a Bayesian Approximation- Representing Model Uncertainty in Deep Learning|
| 11.30  |（直播-Lecture) 概率图模型3: 吉布斯采样||[Lecture35](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E6%A6%82%E7%8E%87%E5%9B%BE%E6%A8%A1%E5%9E%8B3%EF%BC%9A%E5%90%89%E5%B8%83%E6%96%AF%E9%87%87%E6%A0%B7.zip)|||
| 11.30  |（直播-Discussion) LDA模型的实战|模型的使用、吉布斯采样代码讲解|[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/MCMC+LDA_20191130.pdf)|||
| 12.3   |（直播-Lecture) 概率图模型4：Collapsed 吉布斯采样||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/%E8%AF%BE%E4%BB%B6.pptx)|||
| 12.7   |（直播-Discussion) Bayesian SkipGram讲解||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/VI_20191207.pdf)||[Bayesian SkipGram](https://arxiv.org/pdf/1603.06571.pdf)|||
| 12.10  |（直播-Lecture) 概率图模型5：变分法||[Lecture36](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/slide(1).zip)|||
| 12.14  | (直播-Discussion) Bayesian Neural Netowrk讲解与实战||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/12.15.zip)||
| 12.22  |（直播-Lecture) 概率图模型6：变分法，大数据的处理||[Lecture37](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/nlp4.pptx)|||
| 12.28  | (直播-Lecture) 聊天机器人 ||[Lecture38](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/chatbot%20(2).pptx)||[Project4:机器翻译项目](http://47.94.6.102/NLPCamp4/project4)||
| 12.28  | (直播-Discussion) 聊天机器人常见技术综述(1)TC-bot||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/TC-Bot(1).pdf)||||
| 01.04  | (直播-Discussion) 聊天机器人常见技术综述(2)TC-Bot2||[课件](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/01.06%E7%9B%BC%E7%9B%BC%E8%80%81%E5%B8%88/TC-Bot_code.pdf)||||
| 01.11  | (直播-Discussion) 聊天机器人常见技术综述(3)TC-Bot3||[课件](http://47.94.6.102/NLPCamp4/course-info/tree/master/tc-bot3)||[Project5:聊天机器人](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/chatbot)||
| 03.06  | (直播-Discussion) 知识图谱项目讲解||[课件代码](http://47.94.6.102/NLPCamp4/course-info/tree/master/%E8%AF%BE%E4%BB%B6/kg%20project)||||
| 03.06  | (直播-Discussion) 机器翻译项目讲解||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/Project4%EF%BC%8C%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%E9%A1%B9%E7%9B%AE%E5%8F%8A%E7%AD%94%E6%A1%88.zip)||||
| 03.06  | (直播-Discussion) 情感分析项目讲解||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/project2-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90.zip)||||
| 03.06  | (直播-Discussion) 对话系统项目讲解||[课件代码](http://47.94.6.102/NLPCamp4/course-info/blob/master/%E8%AF%BE%E4%BB%B6/project3.zip)||||

